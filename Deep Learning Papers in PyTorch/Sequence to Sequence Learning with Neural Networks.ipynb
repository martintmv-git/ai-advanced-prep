{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Learning with Neural Networks\n",
    "___\n",
    "\n",
    "This notebook represents an implementation of the model described in the referenced paper, which can be accessed [here](https://arxiv.org/pdf/1409.3215v3).\n",
    "\n",
    "The model's architecture is detailed in the **Seq2Seq Model** section, with direct citations from the original paper to ensure accuracy.\n",
    "\n",
    "Training was conducted over $5$ epochs, achieving a final validation loss of approximately $1.00$. This training information is not visible in this notebook due to constraints on output display. Additionally, the computation of the *BLEU* score remains to be incorporated.\n",
    "\n",
    "<small>Note: Certain special characters used for denoting the beginning and end of sentences, as well as padding—specifically the '\\<' and '\\>' symbols—do not display correctly after uploading to GitHub, appearing as empty strings instead.</small>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "\n",
    "# Controlling the randomness in PyTorch and NumPy.\n",
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Dataset\n",
    "I've used a Kaggle dataset. You can download it from [here](https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>FR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          FR\n",
       "0   Hi.      Salut!\n",
       "1  Run!     Cours !\n",
       "2  Run!    Courez !\n",
       "3  Who?       Qui ?\n",
       "4  Wow!  Ça alors !"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = os.path.join(\n",
    "    \"..\", \n",
    "    \"..\", \n",
    "    \"nlp\", \n",
    "    \"datasets\", \n",
    "    \"en-fr-translation\", \n",
    "    \"en-fr.csv\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "df = df.rename(columns={\"English words/sentences\": \"EN\"})\n",
    "df = df.rename(columns={\"French words/sentences\": \"FR\"})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sentence: str):\n",
    "    pattern = r\"([.,!?:;]+)\"\n",
    "    sentence = re.sub(pattern, r\" \\1 \", sentence)\n",
    "\n",
    "    pattern = r\"\\s+\"\n",
    "    sentence = re.sub(pattern, \" \", sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'EN word count'}>,\n",
       "        <AxesSubplot:title={'center':'FR word count'}>]], dtype=object)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"EN word count\"] = df[\"EN\"].apply(lambda x: len(prepare_sentence(x).split(\" \")))\n",
    "df[\"FR word count\"] = df[\"FR\"].apply(lambda x: len(prepare_sentence(x).split(\" \")))\n",
    "df.hist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the distributions, in both languages it would be okay to select $20$ as the max size of an input/output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling and generating a subsample of the dataframe.\n",
    "DATASET_FRACTION = 0.5\n",
    "df = df.sample(frac=DATASET_FRACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should first download these two spaCy models!\n",
    "en_tokenizer = get_tokenizer(\"spacy\", \"en_core_web_sm\")\n",
    "fr_tokenizer = get_tokenizer(\"spacy\", \"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_corpus(corpus: List[str], tokenizer: spacy.tokenizer.Tokenizer, max_len: int):\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenizer(\n",
    "            prepare_sentence(sentence)\n",
    "        )\n",
    "\n",
    "        # Adding padding if it is needed.\n",
    "        if len(tokens) >= max_len:\n",
    "            tokens = tokens[:max_len]\n",
    "        else:\n",
    "            len_diff = max_len - len(tokens)\n",
    "            tokens = tokens + [\"<pad>\"] * len_diff\n",
    "\n",
    "        yield tokens\n",
    "\n",
    "\n",
    "en_corpus = [sent for sent in list(df[\"EN\"])]\n",
    "fr_corpus = [sent for sent in list(df[\"FR\"])]\n",
    "EN_MAX_LEN = 20\n",
    "FR_MAX_LEN = 20\n",
    "SPECIALS = [\"<unk>\", \"<start>\", \"<end>\", \"<pad>\"]\n",
    "\n",
    "en_vocab = build_vocab_from_iterator(\n",
    "    iterate_corpus(en_corpus, en_tokenizer, EN_MAX_LEN), \n",
    "    specials=SPECIALS\n",
    ")\n",
    "en_vocab.set_default_index(en_vocab[\"<unk>\"])\n",
    "\n",
    "fr_vocab = build_vocab_from_iterator(\n",
    "    iterate_corpus(fr_corpus, fr_tokenizer, FR_MAX_LEN), \n",
    "    specials=SPECIALS\n",
    ")\n",
    "fr_vocab.set_default_index(fr_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the 'l1_idx2token' attribute:\n",
      "['<start>', 'I', \"'m\", 'not', 'that', 'cynical', '.', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "x.shape: torch.Size([87810, 22])\n",
      "y.shape: torch.Size([87810, 22])\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        lang1_corpus: List[str], lang2_corpus: List[str],\n",
    "        lang1_tokenizer: spacy.tokenizer.Tokenizer, lang2_tokenizer: spacy.tokenizer.Tokenizer,\n",
    "        lang1_vocab: torchtext.vocab.Vocab, lang2_vocab: torchtext.vocab.Vocab,\n",
    "        lang1_max_len: int = 200, lang2_max_len: int = 200, special_tokens=SPECIALS\n",
    "    ):\n",
    "        self.l1_corpus = lang1_corpus\n",
    "        self.l2_corpus = lang2_corpus\n",
    "\n",
    "        self.l1_max_len = lang1_max_len\n",
    "        self.l2_max_len = lang2_max_len\n",
    "        \n",
    "        self.l1_tokenizer = lang1_tokenizer\n",
    "        self.l2_tokenizer = lang2_tokenizer\n",
    "        \n",
    "        self.l1_vocab = lang1_vocab\n",
    "        self.l2_vocab = lang2_vocab\n",
    "\n",
    "        self.l1_token2idx = lang1_vocab\n",
    "        self.l1_idx2token = {\n",
    "            lang1_vocab[word]: word\n",
    "            for sentence in lang1_corpus\n",
    "            for word in lang1_tokenizer(prepare_sentence(sentence)) + SPECIALS\n",
    "        }\n",
    "        self.l2_token2idx = lang2_vocab\n",
    "        self.l2_idx2token = {\n",
    "            lang2_vocab[word]: word\n",
    "            for sentence in lang2_corpus\n",
    "            for word in lang2_tokenizer(prepare_sentence(sentence)) + SPECIALS\n",
    "        }\n",
    "\n",
    "        self.x, self.y = self._get_x_y()\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def _get_x_y(self):\n",
    "        x = TranslationDataset._parse_corpus(\n",
    "            self.l1_corpus, self.l1_tokenizer, self.l1_vocab, self.l1_max_len\n",
    "        )\n",
    "        y = TranslationDataset._parse_corpus(\n",
    "            self.l2_corpus, self.l2_tokenizer, self.l2_vocab, self.l2_max_len\n",
    "        )\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_corpus(\n",
    "        corpus: List[str], \n",
    "        tokenizer: spacy.tokenizer.Tokenizer, \n",
    "        vocab: torchtext.vocab.Vocab,\n",
    "        max_len: int\n",
    "    ):\n",
    "        output = []\n",
    "\n",
    "        for sent in corpus:\n",
    "            tokens = tokenizer(sent)\n",
    "            indices = [vocab[token] for token in tokens]\n",
    "            if len(indices) >= max_len:\n",
    "                output.append([vocab[\"<start>\"]] + indices[:max_len] + [vocab[\"<end>\"]])\n",
    "            else:\n",
    "                len_diff = max_len - len(indices)\n",
    "                padding = [vocab[\"<pad>\"]] * len_diff\n",
    "                output.append([vocab[\"<start>\"]] + indices + [vocab[\"<end>\"]] + padding)\n",
    "\n",
    "        return torch.LongTensor(output)\n",
    "\n",
    "\n",
    "dataset = TranslationDataset(\n",
    "    lang1_corpus=en_corpus, lang2_corpus=fr_corpus,\n",
    "    lang1_vocab=en_vocab, lang2_vocab=fr_vocab,\n",
    "    lang1_tokenizer=en_tokenizer, lang2_tokenizer=fr_tokenizer,\n",
    "    lang1_max_len=EN_MAX_LEN, lang2_max_len=FR_MAX_LEN\n",
    ")\n",
    "# Printing an example sentence, using the l1_idx2token dictionary.\n",
    "# l1 here means language 1, which is English.\n",
    "x_sample = dataset.x[1]\n",
    "print(\"Using the 'l1_idx2token' attribute:\")\n",
    "print([dataset.l1_idx2token[int(idx)] for idx in x_sample])\n",
    "\n",
    "print(\"x.shape:\", dataset.x.shape)\n",
    "print(\"y.shape:\", dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 70248\n",
      "Validation dataset size: 17562\n"
     ]
    }
   ],
   "source": [
    "def train_validation_split(dataset: torch.utils.data.Dataset, train_size: float):\n",
    "    train_set_size = int(len(dataset) * train_size)\n",
    "    valid_set_size = len(dataset) - train_set_size\n",
    "    datasets_lengths = [train_set_size, valid_set_size]\n",
    "\n",
    "    # Splitting the input dataset into training and validation set.\n",
    "    train_dataset, valid_dataset = random_split(dataset, datasets_lengths)\n",
    "\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "\n",
    "train_dataset, valid_dataset = train_validation_split(\n",
    "    dataset, train_size=0.8\n",
    ")\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(valid_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seq2Seq Model\n",
    "The architecture of this model features two primary components, both based on *LSTM* structures: an *Encoder* and a *Decoder*.\n",
    "\n",
    "- **Encoder**: This component is responsible for processing the input sequence. It first embeds the sequence into a specific space, then passes it through LSTM cells. It outputs not only the final layer's outputs but crucially, the *hidden state*, which is essential for the Decoder's operations.\n",
    "- **Decoder**: Leveraging the *hidden state* received from the Encoder, the Decoder processes the target sequence token by token. During inference, only the '*\\<start\\>*' token is initially provided, and the model generates subsequent tokens one at a time. The Decoder mirrors the Encoder in structure but includes an additional final layer that maps outputs to a vector in $\\mathbb{R}^{vocab}$, representing the vocabulary size (in this case, French).\n",
    "\n",
    "A notable technique implemented from the research paper is the reversal of the input sequence before it reaches the Encoder:\n",
    "> ... we found\n",
    "> it extremely valuable to reverse the order of the words of the input sentence. For example,\n",
    "> rather than mapping the sentence 'a, b, c' to 'α, β, γ', the LSTM is tasked with mapping 'c, b, a' to 'α, β, γ',\n",
    "> where 'α, β, γ' translates 'a, b, c'. This arrangement places 'a' near 'α', 'b' close to 'β', and so on, which facilitates easier learning connections for SGD between the input and the output.\n",
    "\n",
    "Additionally, the Decoder employs *teacher forcing* at a defined probability, determining how often the ground truth values are fed back as input during training.\n",
    "\n",
    "These components and strategies are integrated within the `Seq2Seq` class's `forward()` method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([10, 5, 100])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, num_layers: int, padding_idx: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        embedded_x = self.embed(x)\n",
    "        out, hidden = self.lstm(embedded_x)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "x = torch.randint(low=0, high=20, size=(10, 5))\n",
    "enc = Encoder(vocab_size=50, embed_size=100, hidden_size=100, num_layers=4, padding_idx=0)\n",
    "out, hidden = enc(x)\n",
    "print(f\"Encoder output shape: {out.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([10, 5, 150])\n",
      "Curr. input shape: torch.Size([10])\n",
      "Decoder output shape: torch.Size([10, 1, 200])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, num_layers: int, padding_idx: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.project = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: tuple):\n",
    "        x = x.unsqueeze(0)\n",
    "        x = x.reshape((x.shape[1], x.shape[0]))\n",
    "\n",
    "        embedded_x = self.embed(x)\n",
    "\n",
    "        hidden = [state.detach() for state in hidden]\n",
    "        out, hidden = self.lstm(embedded_x, hidden)\n",
    "        out = self.project(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "# Passing through the Encoder\n",
    "x = torch.randint(low=0, high=20, size=(10, 5))\n",
    "enc = Encoder(vocab_size=50, embed_size=100, hidden_size=150, num_layers=4, padding_idx=0)\n",
    "out, hidden = enc(x)\n",
    "print(f\"Encoder output shape: {out.shape}\")\n",
    "y = torch.randint(low=0, high=40, size=(10, 5))\n",
    "# Sending only the start token to the Decoder.\n",
    "curr_input = y[:, 0]\n",
    "print(f\"Curr. input shape: {curr_input.shape}\")\n",
    "\n",
    "# Passing through the Decoder\n",
    "dec = Decoder(vocab_size=200, embed_size=100, hidden_size=150, num_layers=4, padding_idx=0)\n",
    "out, hidden = dec(curr_input, hidden)\n",
    "print(f\"Decoder output shape: {out.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embed): Embedding(50, 100, padding_idx=0)\n",
      "    (lstm): LSTM(100, 150, num_layers=4, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embed): Embedding(100, 100, padding_idx=0)\n",
      "    (lstm): LSTM(100, 150, num_layers=4, batch_first=True)\n",
      "    (project): Linear(in_features=150, out_features=100, bias=True)\n",
      "  )\n",
      ")\n",
      "x shape: torch.Size([10, 5])\n",
      "y shape: torch.Size([10, 5])\n",
      "Seq2Seq output shape: torch.Size([10, 5, 100])\n"
     ]
    }
   ],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: torch.nn.Module, decoder: torch.nn.Module, reverse_input=True, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.reverse_input = reverse_input\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = y.shape[0], y.shape[1]\n",
    "        vocab_size = self.decoder.vocab_size\n",
    "\n",
    "        # Tensor in which we are going to store the Decoder outputs.\n",
    "        outputs = torch.zeros(size=(batch_size, tgt_len, vocab_size)).to(self.device)\n",
    "\n",
    "        # Reversing the sequences in the input tensor.\n",
    "        # In the paper it's stated that it is extremely valuable, and makes a difference.\n",
    "        if self.reverse_input:\n",
    "            x = torch.flip(x, [1])\n",
    "        \n",
    "        # Here, since the encoder output is not going to be used, I declare it\n",
    "        # as '_'.\n",
    "        _, hidden = self.encoder(x)\n",
    "\n",
    "        # The start token:\n",
    "        curr_input = y[:, 0]\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            out, hidden = self.decoder(curr_input, hidden)\n",
    "            out = out.squeeze(1)\n",
    "\n",
    "            outputs[:, t, :] = out\n",
    "\n",
    "            teacher_force = teacher_forcing_ratio > random.random()\n",
    "\n",
    "            # Getting the top prediction.\n",
    "            top_pred = out.argmax(1)\n",
    "\n",
    "            # If we are in a teacher forcing state - use actual next token\n",
    "            # as an input to the Decoder, else - use the top prediction.\n",
    "            curr_input = y[:, t] if teacher_force else top_pred\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Input tensor:\n",
    "x = torch.randint(low=0, high=20, size=(10, 5))\n",
    "y = torch.randint(low=0, high=40, size=(10, 5))\n",
    "\n",
    "# Defining the Encoder:\n",
    "enc = Encoder(vocab_size=50, embed_size=100, hidden_size=150, num_layers=4, padding_idx=0)\n",
    "\n",
    "# Defining the decoder:\n",
    "dec = Decoder(vocab_size=100, embed_size=100, hidden_size=150, num_layers=4, padding_idx=0)\n",
    "\n",
    "# Creating the Seq2Seq model:\n",
    "model = Seq2Seq(encoder=enc, decoder=dec)\n",
    "out = model(x, y)\n",
    "\n",
    "print(f\"Model:\\n{model}\")\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "print(f\"Seq2Seq output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model: torch.nn.Module):\n",
    "    \"\"\"Initializing the weights with the uniform distribution between -0.08 and \n",
    "    0.08.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model for which the weights will be initialized.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bias\" in name or \"weight\" in name:\n",
    "            torch.nn.init.uniform_(param, a=-0.08, b=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model: torch.nn.Module, path: str):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model: torch.nn.Module, path: str):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "\n",
    "MODEL_FILENAME = \"1-seq2seq.pt\"\n",
    "MODEL_PATH = os.path.join(\"..\", \"models\", MODEL_FILENAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "The training process outlined in this implementation slightly deviates from the methods described in the original paper, though these differences are minimal.\n",
    "\n",
    "The paper specifies the use of a *uniform distribution* for initializing weights:\n",
    "> We initialized all of the LSTM’s parameters with the uniform distribution between -0.08 and 0.08.\n",
    "\n",
    "This initialization is replicated in the `initialize_weights()` function mentioned earlier.\n",
    "\n",
    "> We used stochastic gradient descent without momentum, with a fixed learning rate of 0.7. After 5 epochs, we began halving the learning rate every half epoch. We trained our models for a total of 7.5 epochs.\n",
    "\n",
    "In contrast, this implementation employs the SGD optimizer with an initial learning rate of $0.7$. However, the strategy of halving the learning rate every half epoch is not implemented here.\n",
    "\n",
    "> We used batches of 128 sequences.\n",
    "\n",
    "Due to constraints related to VRAM capacity, this implementation uses a smaller batch size of $64$.\n",
    "\n",
    "> Although LSTMs tend to not suffer from the vanishing gradient problem, they can have exploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10, 25] by scaling it when its norm exceeded a threshold.\n",
    "\n",
    "Similar to the paper, this implementation includes gradient clipping to prevent exploding gradients, utilizing the `torch.nn.utils.clip_grad_norm_()` method as detailed in the `Seq2SeqTrainingSession._train_epoch()`.\n",
    "\n",
    "Lastly, consistent with the paper, the model undergoes training for $5$ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "\n",
    "class Seq2SeqTrainingSession:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: torch.nn.Module, \n",
    "        loss: torch.nn.Module, \n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        epochs: int, batch_size: int, \n",
    "        use_clipping=True,\n",
    "        device=\"cpu\"\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.loss_func = loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.use_clipping = use_clipping\n",
    "        self.clip = 5\n",
    "        self.device = device\n",
    "\n",
    "    def start(\n",
    "        self, \n",
    "        train_dataset: torch.utils.data.Subset, \n",
    "        valid_dataset: torch.utils.data.Subset,\n",
    "        l1_idx2token: dict,\n",
    "        l2_idx2token: dict,\n",
    "        fixed_sentences: List[List[str]] = None\n",
    "    ):\n",
    "        self.fixed_sentences = fixed_sentences\n",
    "\n",
    "        self.l1_idx2token = l1_idx2token\n",
    "        self.l2_idx2token = l2_idx2token\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=0,\n",
    "            drop_last=True\n",
    "        )\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_dataset, \n",
    "            self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=0,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = self._train_epoch(train_dataloader)\n",
    "            valid_loss = self._valid_epoch(valid_dataloader)\n",
    "            print(f\"Epoch: {epoch + 1}, Training Loss: {train_loss:.2f}, Validation Loss: {valid_loss:.2f}\")\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def _train_epoch(self, dataloader):\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "            y_pred = self.model(x, y)\n",
    "\n",
    "            # Here y_pred should be trasformed to shape (BATCH_SIZE * SEQ_LEN, VOCAB_SIZE)\n",
    "            # and y should be transformed to shape (BATCH_SIZE * SEQ_LEN).\n",
    "            loss = self.loss_func(y_pred.reshape((-1, y_pred.shape[-1])), y.reshape(-1))\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Clipping the gradients, since LSTMs can have exploding gradients.\n",
    "            if self.use_clipping:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), \n",
    "                    max_norm=self.clip\n",
    "                )\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if batch_i % 150 == 0:\n",
    "                pred_sents, target_sents = self._tensors2sentences(y_pred[:1], y[:1])\n",
    "                print(f\"Training Batch {batch_i}, Loss: {loss.item():.2f}\")\n",
    "                print(\"Example\")\n",
    "                print(\"-\" * 100)\n",
    "                print(f\"Target: {' '.join(target_sents)}\")\n",
    "                print(f\"Prediction: {' '.join(pred_sents)}\")\n",
    "                print(\"-\" * 100)\n",
    "\n",
    "                # Saving the model checkpoint.\n",
    "                save_checkpoint(model=self.model, path=MODEL_PATH)\n",
    "\n",
    "        epoch_loss = loss.item()\n",
    "        # TODO: Fix the calculation of BLEU.\n",
    "        # epoch_bleu = self._calc_bleu(y_pred, y)\n",
    "\n",
    "        return epoch_loss\n",
    "\n",
    "    def _valid_epoch(self, dataloader):\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_i, (x, y) in enumerate(dataloader):\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "            \n",
    "                y_pred = self.model(x, y)\n",
    "\n",
    "                loss = self.loss_func(y_pred.reshape((-1, y_pred.shape[-1])), y.reshape(-1))\n",
    "\n",
    "            epoch_loss = loss.item()\n",
    "            # TODO: Fix the calculation of BLEU.\n",
    "            # epoch_bleu = self._calc_bleu(y_pred, y)\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        return epoch_loss\n",
    "\n",
    "    def _tensors2sentences(self, y_pred: torch.Tensor, y):\n",
    "        predictions = y_pred.argmax(-1).tolist()\n",
    "        targets = y.tolist()\n",
    "        predictions = [\n",
    "            self.l2_idx2token[idx] \n",
    "            for sent in predictions \n",
    "            for idx in sent\n",
    "        ]\n",
    "        targets = [\n",
    "            self.l2_idx2token[idx] \n",
    "            for sent in targets \n",
    "            for idx in sent\n",
    "        ]\n",
    "\n",
    "        return predictions, targets\n",
    "\n",
    "    def _calc_bleu(self, y_pred, y):\n",
    "        predictions = y_pred.argmax(-1).tolist()\n",
    "        targets = y.tolist()\n",
    "\n",
    "        prediction_words = []\n",
    "        target_words = []\n",
    "        for i, (pred_sent, targ_sent) in enumerate(zip(predictions, targets)):\n",
    "            prediction_words.append([])\n",
    "            target_words.append([])\n",
    "            for pred_idx, targ_idx in zip(pred_sent, targ_sent):\n",
    "                prediction_words[i].append(self.l2_idx2token[pred_idx])\n",
    "                target_words[i].append(self.l2_idx2token[targ_idx])\n",
    "\n",
    "        return bleu_score(prediction_words, target_words)\n",
    "\n",
    "\n",
    "# The paper uses 1000, but their dataset is huge.\n",
    "EMBED_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 4\n",
    "# The batch size was 128 in the paper, but my GPU has 2GB VRAM :[.\n",
    "BATCH_SIZE = 64\n",
    "L_RATE = 1e-1\n",
    "# The epochs in the paper are 7.5 (7 epochs and a half).\n",
    "# The model and the dataset are somewhat different, so I'll use different\n",
    "# number of epochs.\n",
    "EPOCHS = 5\n",
    "\n",
    "LOAD_MODEL = False\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "LOG_PATH = \"../logs/1-seq2seq.txt\"\n",
    "\n",
    "\n",
    "enc = Encoder(\n",
    "    vocab_size=len(en_vocab),\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    padding_idx=en_vocab[\"<pad>\"]\n",
    ")\n",
    "\n",
    "dec = Decoder(\n",
    "    vocab_size=len(fr_vocab),\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    padding_idx=fr_vocab[\"<pad>\"]\n",
    ")\n",
    "\n",
    "model = Seq2Seq(\n",
    "    encoder=enc, decoder=dec, \n",
    "    reverse_input=True,\n",
    "    device=DEVICE\n",
    ").to(DEVICE)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model = load_checkpoint(model, path=MODEL_PATH)\n",
    "else:\n",
    "    model.apply(initialize_weights)\n",
    "\n",
    "# The loss function and the optimizer.\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=L_RATE)\n",
    "\n",
    "training_session = Seq2SeqTrainingSession(\n",
    "    model=model,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    use_clipping=True,\n",
    "    device=DEVICE\n",
    ")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "training_session.start(\n",
    "    train_dataset=train_dataset, \n",
    "    valid_dataset=valid_dataset,\n",
    "    l1_idx2token=dataset.l1_idx2token,\n",
    "    l2_idx2token=dataset.l2_idx2token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the model output to a logging file, since the log is quite large.\n",
    "with open(LOG_PATH, 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(cap.stdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8100bb27ef6f27bb6b63ba202e13f32f0dffed430e6a4d162d3986e448f218b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
