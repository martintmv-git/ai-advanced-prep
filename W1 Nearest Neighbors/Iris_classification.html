<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>5e035f9d4acf4fc29533dbb43cf8f043</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell markdown">
<h1 style="font-size:300#">Iris classification with Nearest Neigbors</h1>
<p>BSHT Michielsen MSc</p>
<p>This notebook demonstrates how to use the <a
href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">sklearn
Nearest Neigbours classifier</a> algorithm to classify iris flowers
based on simple mearements of their petals and sepals. The iris is a
flower with a particular appearance because it has different parts that
are shaped significantly different among the three typical species, see
the image below. Therefore, it is relatively easy for a human to
identify which species of iris we are dealing with based on the shape of
those parts, and it is also relatively easy to let a machine classify
them using the same metrics. Note that is a classification problem, so
we use the <code>classifier</code> variety of nearest neighbors. A
<code>regression</code> variety exists as well for other problems.</p>
<p><img src="https://machinelearninghd.com/wp-content/uploads/2021/03/iris-dataset.png" /></p>
<p>First, we show the versions of the required libraries (that is always
wise to do in case you have to report problems running the
notebooks!).</p>
</div>
<div class="cell code" data-execution_count="3">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install seaborn <span class="co">#adding seaborn pkg to the notebook cuz I was missing it</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>Collecting seaborn
  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)
Requirement already satisfied: numpy!=1.24.0,&gt;=1.20 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from seaborn) (1.23.2)
Requirement already satisfied: pandas&gt;=1.2 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from seaborn) (2.0.3)
Requirement already satisfied: matplotlib!=3.6.1,&gt;=3.4 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from seaborn) (3.7.2)
Requirement already satisfied: contourpy&gt;=1.0.1 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (1.0.5)
Requirement already satisfied: cycler&gt;=0.10 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (0.11.0)
Requirement already satisfied: fonttools&gt;=4.22.0 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (4.25.0)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (1.4.4)
Requirement already satisfied: packaging&gt;=20.0 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (23.1)
Requirement already satisfied: pillow&gt;=6.2.0 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (10.0.1)
Requirement already satisfied: pyparsing&lt;3.1,&gt;=2.3.1 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (3.0.9)
Requirement already satisfied: python-dateutil&gt;=2.7 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (2.8.2)
Requirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (6.1.1)
Requirement already satisfied: pytz&gt;=2020.1 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3.post1)
Requirement already satisfied: tzdata&gt;=2022.1 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3)
Requirement already satisfied: zipp&gt;=3.1.0 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (3.17.0)
Requirement already satisfied: six&gt;=1.5 in /Users/martintomov/tensorflowtesting/env/lib/python3.8/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (1.16.0)
Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.9/294.9 kB 4.8 MB/s eta 0:00:00a 0:00:01
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="4" data-scrolled="true">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn <span class="im">as</span> sk</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;scikit-learn version:&quot;</span>, sk.__version__)          <span class="co"># 1.1.1</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;numpy version:&quot;</span>, np.__version__)                 <span class="co"># 1.22.4</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;pandas version:&quot;</span>, pd.__version__)                <span class="co"># 1.4.2</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;seaborn version:&quot;</span>, sns.__version__)              <span class="co"># 0.11.2</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>scikit-learn version: 1.3.0
numpy version: 1.23.2
pandas version: 2.0.3
seaborn version: 0.13.2
</code></pre>
</div>
</div>
<section id="data-provisioning" class="cell markdown">
<h1>Data provisioning</h1>
<p>In real life your data provisioning is likely to include more steps
about data sourcing and data quality, however for demo purposes in this
notebook we limit it to merely loading the data using the <a
href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html">pandas
read_csv</a> function, without any concern over quantity nor quality.
The data has 150 observations in 5 columns namely 4 features and 1
target variable.</p>
</section>
<div class="cell code" data-execution_count="5">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>columns <span class="op">=</span> [<span class="st">&quot;Sepal Length&quot;</span>, <span class="st">&quot;Sepal Width&quot;</span>, <span class="st">&quot;Petal Length&quot;</span>, <span class="st">&quot;Petal Width&quot;</span>, <span class="st">&quot;Species&quot;</span>]</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot;</span>, names<span class="op">=</span>columns)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.shape)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>(150, 5)
</code></pre>
</div>
</div>
<section id="sample-the-data" class="cell markdown">
<h1>Sample the data</h1>
<p>To get an impression of the data, here we take a sample of 10
observations from the set and have a look at the features. The idea here
is that a certain combination of values for the features is more likely
to mean a certain species. For example <code>iris-setosa</code> seems to
have a relatively small <code>Petal Width</code> and
<code>iris-versicolor</code> has a relatively high
<code>Sepal Length</code>. Knowing this, one could, given the lengths
and widths for sepal and petal make an educated guess of the species by
looking at how close its values are to already existing observations.
This is the idea of Nearest Neighbors, it looks for "how close" a new
observation is to previous known observations. In the second part we
look at how many observations there are for each of the classes in the
target variable. Because this dataset is very balanced there are exacly
50 observations for each of the 3 classes.</p>
</section>
<div class="cell code" data-execution_count="6">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>df.sample(<span class="dv">10</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="6">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sepal Length</th>
      <th>Sepal Width</th>
      <th>Petal Length</th>
      <th>Petal Width</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>98</th>
      <td>5.1</td>
      <td>2.5</td>
      <td>3.0</td>
      <td>1.1</td>
      <td>Iris-versicolor</td>
    </tr>
    <tr>
      <th>58</th>
      <td>6.6</td>
      <td>2.9</td>
      <td>4.6</td>
      <td>1.3</td>
      <td>Iris-versicolor</td>
    </tr>
    <tr>
      <th>95</th>
      <td>5.7</td>
      <td>3.0</td>
      <td>4.2</td>
      <td>1.2</td>
      <td>Iris-versicolor</td>
    </tr>
    <tr>
      <th>142</th>
      <td>5.8</td>
      <td>2.7</td>
      <td>5.1</td>
      <td>1.9</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>91</th>
      <td>6.1</td>
      <td>3.0</td>
      <td>4.6</td>
      <td>1.4</td>
      <td>Iris-versicolor</td>
    </tr>
    <tr>
      <th>134</th>
      <td>6.1</td>
      <td>2.6</td>
      <td>5.6</td>
      <td>1.4</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>85</th>
      <td>6.0</td>
      <td>3.4</td>
      <td>4.5</td>
      <td>1.6</td>
      <td>Iris-versicolor</td>
    </tr>
    <tr>
      <th>64</th>
      <td>5.6</td>
      <td>2.9</td>
      <td>3.6</td>
      <td>1.3</td>
      <td>Iris-versicolor</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code" data-execution_count="7">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>groupinfo <span class="op">=</span> df.groupby([<span class="st">&quot;Species&quot;</span>])[<span class="st">&quot;Species&quot;</span>].count()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(groupinfo)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Species
Iris-setosa        50
Iris-versicolor    50
Iris-virginica     50
Name: Species, dtype: int64
</code></pre>
</div>
</div>
<section id="preprocessing" class="cell markdown">
<h1>Preprocessing</h1>
<p>The step of preprocessing contains several activities to consider
before we can start training the algorithm and produce a model that can
predict our target variable, in this case the species of an iris
flower.</p>
<h3 id="target-variable">Target variable</h3>
<p>The first thing to do here would be making sure that we have a
numeric target variable. Algorithms work only with numbers and therefore
can only give a number as a result. Let's make a new column
<code>Species ID</code> that is a number for the <code>Species</code>.
It does not really matter which numbers we pick, so let's do 1, 2 and 3,
for lack of inspiration. 🧐</p>
</section>
<div class="cell code" data-execution_count="8">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&quot;Species ID&quot;</span>] <span class="op">=</span> df[<span class="st">&quot;Species&quot;</span>].<span class="bu">map</span>({<span class="st">&quot;Iris-setosa&quot;</span>: <span class="dv">1</span>, <span class="st">&quot;Iris-versicolor&quot;</span>: <span class="dv">2</span>, <span class="st">&quot;Iris-virginica&quot;</span>: <span class="dv">3</span>})</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>df.sample(<span class="dv">10</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="8">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sepal Length</th>
      <th>Sepal Width</th>
      <th>Petal Length</th>
      <th>Petal Width</th>
      <th>Species</th>
      <th>Species ID</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
      <td>1</td>
    </tr>
    <tr>
      <th>40</th>
      <td>5.0</td>
      <td>3.5</td>
      <td>1.3</td>
      <td>0.3</td>
      <td>Iris-setosa</td>
      <td>1</td>
    </tr>
    <tr>
      <th>117</th>
      <td>7.7</td>
      <td>3.8</td>
      <td>6.7</td>
      <td>2.2</td>
      <td>Iris-virginica</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>5.4</td>
      <td>3.4</td>
      <td>1.5</td>
      <td>0.4</td>
      <td>Iris-setosa</td>
      <td>1</td>
    </tr>
    <tr>
      <th>103</th>
      <td>6.3</td>
      <td>2.9</td>
      <td>5.6</td>
      <td>1.8</td>
      <td>Iris-virginica</td>
      <td>3</td>
    </tr>
    <tr>
      <th>86</th>
      <td>6.7</td>
      <td>3.1</td>
      <td>4.7</td>
      <td>1.5</td>
      <td>Iris-versicolor</td>
      <td>2</td>
    </tr>
    <tr>
      <th>94</th>
      <td>5.6</td>
      <td>2.7</td>
      <td>4.2</td>
      <td>1.3</td>
      <td>Iris-versicolor</td>
      <td>2</td>
    </tr>
    <tr>
      <th>10</th>
      <td>5.4</td>
      <td>3.7</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
      <td>1</td>
    </tr>
    <tr>
      <th>126</th>
      <td>6.2</td>
      <td>2.8</td>
      <td>4.8</td>
      <td>1.8</td>
      <td>Iris-virginica</td>
      <td>3</td>
    </tr>
    <tr>
      <th>128</th>
      <td>6.4</td>
      <td>2.8</td>
      <td>5.6</td>
      <td>2.1</td>
      <td>Iris-virginica</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="feature-selection" class="cell markdown">
<h3>Feature selection</h3>
<p>Perhaps the most important step is 'feature selection', which is the
activity of deciding which of the available features to use in order to
predict the target variable. A common mistake is to just take all
features "because knowing more, implies a higher chance of guessing
correct", which is not true. Sometimes some features do not help at all,
and some even produce noise in the algorithm. It is therefore important
to pick only those features that can together explain the variance in
the target variable the best. For a classification algorithm that would
mean that any feature that has distinguishable values per class would be
good. A way to look for this is by creating a <a
href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html">pandas
box plot</a>.</p>
</section>
<div class="cell code" data-execution_count="9">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> columns[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plot <span class="op">=</span> df.boxplot(column<span class="op">=</span>features, by<span class="op">=</span><span class="st">&quot;Species&quot;</span>, figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">7</span>), layout<span class="op">=</span>(<span class="dv">1</span>,<span class="bu">len</span>(features)))</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_0b376a3f9c2b4053a12e83e2744073bb/2b60e8de9b0ceb7af378f08e88c521ea83f783b7.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>The idea here is that if the boxes are vertically close to each
other, that feature would be less good because the values of that
feature are close to each other and therefore hard to distinguish. A
feature in which the boxes are vertically far away from each other would
be a good feature. Now, in this dataset any feature is still a fair
choice, but for example <code>Petal Length</code>, having boxes far away
from each other, would be a better choice than <code>Sepal Width</code>
where the boxes are closer to each other. Also <code>Petal Width</code>
seems to be a fair choice. The selected features we put into a variable
named <code>X</code> and the target variable we put in a variable named
<code>y</code>.</p>
</div>
<div class="cell code" data-execution_count="27">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[[<span class="st">&quot;Petal Length&quot;</span>, <span class="st">&quot;Petal Width&quot;</span>]]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">&quot;Species ID&quot;</span>]</span></code></pre></div>
</div>
<section id="splitting-traintest" class="cell markdown">
<h3>Splitting train/test</h3>
<p>Before we can train the model, we need to take a little part of our
data that we use for testing purposes. The idea here is that the model
trains with, for example 80% of the data that we have, and we use the
other 20% to ask it to predict the target variable for. Now, because we
know the true target variable of that 20% we can compare the results
that the model gives us with the ground truth and come up with how well
the model performs. Let's make a train/test split now.</p>
</section>
<div class="cell code" data-execution_count="11">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;There are in total&quot;</span>, <span class="bu">len</span>(X), <span class="st">&quot;observations.&quot;</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;The train set now has&quot;</span>, <span class="bu">len</span>(X_train), <span class="st">&quot;observations.&quot;</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;The trest set now has&quot;</span>, <span class="bu">len</span>(X_test), <span class="st">&quot;observations.&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>There are in total 150 observations.
The train set now has 120 observations.
The trest set now has 30 observations.
</code></pre>
</div>
</div>
<section id="scaling" class="cell markdown">
<h3>Scaling</h3>
<p>The Nearest Neigbors algorithm uses the concept of "being near" as to
decide which species an unknown iris would be. This "being near" is
calculated using <a
href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidian
distance</a>, which is an absolute measurement between two values of the
same feature, but not among features. It is therefore needed to scale
all the features such that they use the same unit of measurement. Here
is an example that demonstrates why:</p>
<blockquote>
<p>Given the numbers 6 and 8, the Euclidian distance would be 2. Given
the numbers 95 and 100 the Euclidian distance would be 5. However, 95
and 100 are in fact closer to eachother (95%) than 6 and 8 are
(75%).</p>
</blockquote>
<p>Therefore it is important that among features the same unit is used.
A common unit to use is <a
href="https://en.wikipedia.org/wiki/Standard_deviation">standard
deviation</a>, and sklearn provides the <code>StandardScaler</code> to
transform all values into that same unit. After that we can make a
scatter plot to see the actual distances between the iris flowers. Note
that the distance is in standard deviations (σ).</p>
</section>
<div class="cell code" data-execution_count="12">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>scaler.fit(X_train)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> scaler.transform(X_train)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> scaler.transform(X_test)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(rc<span class="op">=</span>{<span class="st">&quot;figure.figsize&quot;</span>:(<span class="dv">10</span>, <span class="dv">6</span>)})</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.scatterplot(x<span class="op">=</span>X_train[:,<span class="dv">0</span>], y<span class="op">=</span>X_train[:,<span class="dv">1</span>], hue<span class="op">=</span>y_train, palette<span class="op">=</span><span class="st">&quot;viridis&quot;</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlabel<span class="op">=</span>X.columns[<span class="dv">0</span>], ylabel<span class="op">=</span>X.columns[<span class="dv">1</span>], title<span class="op">=</span><span class="st">&quot;Distance between iris flowers (σ)&quot;</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>ax.legend(ax.legend_.legendHandles, df[<span class="st">&quot;Species&quot;</span>].unique(), title<span class="op">=</span><span class="st">&quot;Species&quot;</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>/var/folders/bn/xzwp15h96jg7vf7qxvsqkry80000gn/T/ipykernel_49479/2942353307.py:10: MatplotlibDeprecationWarning: The legendHandles attribute was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use legend_handles instead.
  ax.legend(ax.legend_.legendHandles, df[&quot;Species&quot;].unique(), title=&quot;Species&quot;)
</code></pre>
</div>
<div class="output execute_result" data-execution_count="12">
<pre><code>&lt;matplotlib.legend.Legend at 0x126a11bb0&gt;</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0b376a3f9c2b4053a12e83e2744073bb/cc2c3691eaf4fccdfe277d807db26209eca860bc.png" /></p>
</div>
</div>
<section id="modelling" class="cell markdown">
<h1>Modelling</h1>
<p>In this step we are going to use the train set only to fit the model,
in this case Nearest Neigbors for classification named <a
href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">kNeighborsClassifier</a>.
And then we use the test set to calculate the model's accuracy, in other
words how well it performs. Accuracy is a fraction where any value
closer to 1 is considered better, and 1 itself (100% accurate) is
usually impossible.</p>
</section>
<div class="cell code" data-execution_count="13">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> KNeighborsClassifier()</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> accuracy_score(pred, y_test)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Accuracy:&quot;</span>, acc)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Accuracy: 0.9666666666666667
</code></pre>
</div>
</div>
<section id="evaluation" class="cell markdown">
<h1>Evaluation</h1>
<p>Now let us see if we can shed some light on the results. What we can
do here is print a classification report. This shows for every one of
the classes how well the model performed.</p>
</section>
<div class="cell code" data-execution_count="14">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>report <span class="op">=</span> classification_report(y_test, pred, target_names<span class="op">=</span>df[<span class="st">&quot;Species&quot;</span>].unique())</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(report)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>                 precision    recall  f1-score   support

    Iris-setosa       1.00      1.00      1.00         8
Iris-versicolor       0.92      1.00      0.96        11
 Iris-virginica       1.00      0.91      0.95        11

       accuracy                           0.97        30
      macro avg       0.97      0.97      0.97        30
   weighted avg       0.97      0.97      0.97        30

</code></pre>
</div>
</div>
<section id="inference-example" class="cell markdown">
<h1>Inference Example</h1>
<p>Now that the model is trained for predicting iris species based on a
given petal length and petal width, it can now be used to inference a
class for a new observation. This means that for any given combination
of petal length and petal width it can give a probability of how likely
that combination indicates the classes that the model knows. Feel free
to change the values for <code>petal_length</code> and
<code>petal_width</code> below to get another prediction. Note that the
values provided are in cm, but the model was trained on standard
deviations. Therefore the same scaler is used again to transform the
given values into standard deviations before feeding the values to the
model.</p>
</section>
<div class="cell code" data-execution_count="16">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>petal_length <span class="op">=</span> <span class="fl">5.5</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>petal_width <span class="op">=</span> <span class="fl">1.5</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>X_inference <span class="op">=</span> pd.DataFrame([{<span class="st">&quot;Petal Length&quot;</span>: petal_length, <span class="st">&quot;Petal Width&quot;</span>: petal_width}])</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>X_inference <span class="op">=</span> scaler.transform(X_inference)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> model.predict_proba(X_inference)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> pd.DataFrame()</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>result[<span class="st">&quot;Class&quot;</span>] <span class="op">=</span> df[<span class="st">&quot;Species&quot;</span>].unique()</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>result[<span class="st">&quot;Probability&quot;</span>] <span class="op">=</span> prediction[<span class="dv">0</span>]</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>result</span></code></pre></div>
<div class="output execute_result" data-execution_count="16">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Class</th>
      <th>Probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Iris-setosa</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Iris-versicolor</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Iris-virginica</td>
      <td>0.6</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="feature-selection---task-1" class="cell markdown">
<h2>Feature selection - Task 1</h2>
<p>In the section about feature selection we selected
<code>Petal Length</code> and <code>Petal Width</code> as our two
features to base the modelling on. Pick any other two features and
compare the results. Did it work beter? If yes, why, if no, why? What
happens if you select 3 features?</p>
</section>
<div class="cell code" data-execution_count="36">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature selection - 2 new features</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">&quot;Sepal Length&quot;</span>, <span class="st">&quot;Sepal Width&quot;</span>]</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Splitting the dataset into training and testing sets</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardizing the features</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>scaler.fit(X_train)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> scaler.transform(X_train)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> scaler.transform(X_test)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Building and training the KNeighborsClassifier model</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> KNeighborsClassifier()</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a><span class="co"># And finally evaluating the model&#39;s performance</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> accuracy_score(pred, y_test)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Accuracy with new features:&quot;</span>, score)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Accuracy with new features: 0.9333333333333333
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="39">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature selection - all 4 features</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">&quot;Petal Length&quot;</span>, <span class="st">&quot;Petal Width&quot;</span>, <span class="st">&quot;Sepal Length&quot;</span>, <span class="st">&quot;Sepal Width&quot;</span>] </span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Splitting the dataset into training and testing sets</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardizing the features</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>scaler.fit(X_train)</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> scaler.transform(X_train)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> scaler.transform(X_test)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Building and training the KNeighborsClassifier model</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> KNeighborsClassifier()</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a><span class="co"># And finally evaluating the model&#39;s performance</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> accuracy_score(pred, y_test)</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Accuracy with new features:&quot;</span>, score)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Accuracy with new features: 1.0
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<h2 id="feature-selection---results-comparison">Feature selection -
Results comparison</h2>
</blockquote>
<p>I selected the septal length and width to test them out. The output
shows an accuracy of <code>0.9333333333333333</code> or about
<code>93.33%</code> with <code>Sepal Length</code> and
<code>Sepal Width</code> as features.</p>
<p>The previous result of the model accuracy before setting
<code>Sepal Length</code> and <code>Sepal Width</code> as features the
accuracy was approximately <code>96.67%</code>. Since this is higher
than the <code>93.33%</code> accuracy obtained, we can conclude that
<code>Petal Length</code> and <code>Petal Width</code> provided a better
basis for the model to classify the iris species. This could be due to
these features having a stronger connection with the species
classification in the <code>Iris dataset</code>, which leads to a more
accurate model. The septals of the flowers are more similar than their
petals so its harder to recognise the differences this way.</p>
<p>When I added a <code>third feature</code> I got even better and
consistent results which is expected, and if you use all
<code>4 features</code> you get the most consistent results with regular
accuracy score of <b>1.0 = 100%</b>.</p>
</div>
<section id="different-results---task-2" class="cell markdown">
<h2>Different results - Task 2</h2>
<p>If you re-run this notebook a few times, without changing anything,
you sometimes get slightly different results. Explain why you believe
this is. Compare your answer with a fellow student. Did they have the
same opinion? If not, explain the difference.</p>
</section>
<div class="cell code" data-execution_count="37">
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating a classification report with features = [&quot;Sepal Length&quot;, &quot;Sepal Width&quot;]</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>report <span class="op">=</span> classification_report(y_test, pred, target_names<span class="op">=</span>df[<span class="st">&quot;Species&quot;</span>].unique())</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(report)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>                 precision    recall  f1-score   support

    Iris-setosa       1.00      1.00      1.00        10
Iris-versicolor       0.71      1.00      0.83         5
 Iris-virginica       1.00      0.87      0.93        15

       accuracy                           0.93        30
      macro avg       0.90      0.96      0.92        30
   weighted avg       0.95      0.93      0.94        30

</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="40">
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating a classification report with features = [&quot;Petal Length&quot;, &quot;Petal Width&quot;, &quot;Sepal Length&quot;, &quot;Sepal Width&quot;] </span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>report <span class="op">=</span> classification_report(y_test, pred, target_names<span class="op">=</span>df[<span class="st">&quot;Species&quot;</span>].unique())</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(report)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>                 precision    recall  f1-score   support

    Iris-setosa       1.00      1.00      1.00         8
Iris-versicolor       1.00      1.00      1.00         8
 Iris-virginica       1.00      1.00      1.00        14

       accuracy                           1.00        30
      macro avg       1.00      1.00      1.00        30
   weighted avg       1.00      1.00      1.00        30

</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<h2 id="reason-for-different-results">Reason for Different Results</h2>
</blockquote>
<ol>
<li>The k-NN algorithm shuffles the data during training and this can
lead to different results each time.</li>
<li>It's likely because the <code>train_test_split</code> function
randomly partitions the dataset each time it's called unless a
<code>random_state</code> is set.</li>
</ol>
</div>
<section id="hyperparameter-n_neighbors" class="cell markdown">
<h3>Hyperparameter n_neighbors</h3>
<p>The constructor of the kNeighborsClassifier has an optional
hyperparameter named <code>n_neighbors</code>, which by default is
<code>5</code>. Try providing a value of <code>2</code> for this
parameter and see if this makes any difference. Then provide a value of
<code>85</code> for this parameter and note any differences. Which of
those 3 values [<code>5</code>, <code>2</code>, <code>85</code>] would
you say gives the best results and why?</p>
</section>
<div class="cell code" data-execution_count="43">
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For n_neighbors = 2</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>model_n2 <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>model_n2.fit(X_train, y_train)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>pred_n2 <span class="op">=</span> model_n2.predict(X_test)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>score_n2 <span class="op">=</span> accuracy_score(pred_n2, y_test)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># For n_neighbors = 85</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>model_n85 <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">85</span>)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>model_n85.fit(X_train, y_train)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>pred_n85 <span class="op">=</span> model_n85.predict(X_test)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>score_n85 <span class="op">=</span> accuracy_score(pred_n85, y_test)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the accuracy for each model</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Accuracy (n_neighbors=2):&quot;</span>, score_n2)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Accuracy (n_neighbors=5):&quot;</span>, score)  <span class="co"># Default n_neighbors=5</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Accuracy (n_neighbors=85):&quot;</span>, score_n85)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Accuracy (n_neighbors=2): 0.9
Accuracy (n_neighbors=5): 1.0
Accuracy (n_neighbors=85): 0.5333333333333333
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<h2 id="hyperparameter-n_neighbors-results-analysis">Hyperparameter
n_neighbors Results Analysis:</h2>
<p>A lower n_neighbors value means that the model is considering a
smaller number of nearest points, which can make it sensitive to noise
but also potentially more adaptable to local variations. A very high
n_neighbors value (like 85) can make the model too generalized, as it's
considering a very large neighborhood for making predictions, which
might dilute the influence of the closest and most relevant points. In
this case, either 2 or 5 gives the best results because 85 leads to
over-generalization and a loss of accuracy.</p>
</blockquote>
<p>Among the three values <code>5</code>, <code>2</code>, and
<code>85</code> for the <code>n_neighbors</code> hyperparameter, the
choice that gives the best results is <code>5</code>.</p>
</div>
</body>
</html>
