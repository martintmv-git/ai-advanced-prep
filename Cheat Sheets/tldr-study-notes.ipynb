{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>TLDR Notes:</b> ML, workflow and tech stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "#### Data collection is the first and foundational step in the machine learning pipeline. It involves gathering `raw data` from various sources which could include `sensors, logs, databases, datasets, user inputs,` or `online repositories`. The quality and quantity of collected data can significantly influence the performance of ML models. Techniques range from `simple data scraping` to `complex data streaming`, and technologies often used include SQL for databases, APIs for web services, and specialized hardware for sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "#### This process involves cleaning and transforming raw data into a format that ML algorithms can understand. It's about `handling missing values`, `encoding categorical variables`, `normalizing` or `scaling numerical values`, and `feature engineering`. Tools like `pandas` in Python are commonly used. Proper data preparation can greatly enhance model accuracy and is often considered the most time-consuming part of the ML process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms\n",
    "#### These are the set of rules and techniques that allow computers to find patterns and make decisions based on data. From simple `linear regression` and `decision trees` to `complex neural networks` and `ensemble methods`, each algorithm has its strengths and is chosen based on the specific problem and data type. Libraries like [scikit-learn](https://scikit-learn.org/stable/), [TensorFlow](https://www.tensorflow.org), and [PyTorch](https://pytorch.org) provide implementations of these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization in ML\n",
    "#### It's a powerful practice to explore and communicate data insights through graphical representation. Tools like [Matplotlib](https://matplotlib.org), [Seaborn](https://seaborn.pydata.org), and [Plotly](https://plotly.com) in Python, or ggplot2 in R, are used to create charts, graphs, and interactive plots. Good visualization helps in understanding complex data, detecting outliers, errors, and patterns, and is crucial for communicating findings to stakeholders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Programming for ML\n",
    "#### Python is a well-known, high-level language widely adopted in machine learning for its simplicity and the extensive availability of libraries and frameworks. Libraries like [NumPy](https://numpy.org) for numerical operations, [pandas](https://pandas.pydata.org) for data manipulation, [scikit-learn](https://scikit-learn.org/stable/) for machine learning, and [TensorFlow](https://www.tensorflow.org) and [PyTorch](https://pytorch.org) for deep learning, form the core stack for ML in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebooks for ML\n",
    "#### Jupyter Notebooks offer an interactive coding environment where you can write and execute code, visualize data, and document the process using Markdown. It's highly favored for ML projects due to its ability to combine code, output, and annotations into a single document, making it ideal for experiments, exploratory data analysis, and educational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Tech Stack\n",
    "\n",
    "- #### `NumPy` is used for handling numerical operations. Its array objects are much faster and compact than traditional Python lists. NumPy arrays form the core structure that pandas and other libraries build upon.\n",
    "\n",
    "- #### `Pandas` is utilized for data manipulation and analysis. It offers data structures like DataFrames, which make it easy to load, manage, and manipulate tabular data with ease. Pandas is typically used for data cleaning, filtration, and transformation tasks.\n",
    "\n",
    "- #### `Matplotlib` is employed for creating static, interactive, and animated visualizations in Python. It's useful for plotting graphs and charts, which are essential for data exploration and results presentation.\n",
    "\n",
    "- #### Libraries like `Seaborn build on Matplotlib`, offering a higher-level interface for drawing attractive and informative statistical graphics. It's often used alongside pandas for seamless data visualization tasks.\n",
    "\n",
    "- #### `Scikit-learn` is the go-to library for classical machine learning algorithms. It's used for tasks ranging from preprocessing data, feature extraction, and modeling with algorithms like linear regression, decision trees, and clustering.\n",
    "\n",
    "- #### `TensorFlow` is a powerful library for deep learning developed by Google. It's used extensively for constructing and training neural networks with large datasets, often for applications in image and speech recognition, natural language processing, and more.\n",
    "\n",
    "- #### `PyTorch`, created by Meta, is another library for deep learning. It's known for its flexibility and dynamic computational graph, which is particularly friendly for research and development. It allows for easy and fast adjustments to neural networks, making it a preferred choice for experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: \n",
    "####  It's a method to predict a dependent variable based on the independent variable's value, assuming a linear relationship between them. For instance, predicting a car's fuel efficiency (miles per gallon) based on its engine size.\n",
    "\n",
    "# Feature Scaling: \n",
    "#### This process ensures all numerical features have a similar scale, preventing models from misinterpreting the data due to the sheer difference in magnitude. An example is adjusting the scales of income and age when predicting credit scores, so both features contribute equally to the model.\n",
    "\n",
    "# Feature Engineering: \n",
    "#### It involves creating new features from existing ones to improve model performance. For instance, from a dataset with date and time of purchases, creating a new feature that represents the day of the week could reveal weekly patterns in purchase behavior.\n",
    "\n",
    "# Supervised vs. Unsupervised Learning: \n",
    "#### Supervised learning involves learning a function that maps inputs to outputs based on example input-output pairs (e.g., classifying emails into spam or not spam). Unsupervised learning finds patterns or structures in data without explicitly provided outputs (e.g., segmenting customers into clusters based on purchasing behavior).\n",
    "\n",
    "# Reinforcement Learning: \n",
    "#### It's about learning what actions to take in a given situation to maximize a reward. For example, in automated trading systems, reinforcement learning can be used to decide whether to buy, hold, or sell a stock based on the current market state to maximize profit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Concepts:\n",
    "## 1. Supervised Learning:\n",
    "#### Uses labeled data\n",
    "> #### Key concepts: Regression, Classification \n",
    "## 2. Unsupervised Learning:\n",
    "#### Finds patterns in unlabeled data\n",
    "> #### Key concepts: Clustering, Dimensionality Reduction \n",
    "## 3. Semi-supervised Learning:\n",
    "#### Mixes labeled and unlabeled data\n",
    "> #### Key concepts: Combination of supervised and unsupervised techniques \n",
    "## 4. Reinforcement Learning:\n",
    "#### Learns through trial and error\n",
    "> #### Key concepts: Agents, Environment, Actions, Rewards \n",
    "## 5. Deep Learning:\n",
    "#### Neural networks for unstructured data\n",
    "> #### Key concepts: Neural Networks, CNNs, RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "#### An optimization algorithm that minimizes the cost function, akin to finding the lowest point in a valley. \n",
    "> #### Example: Adjusting the slope and intercept in linear regression to reduce prediction error.\n",
    "\n",
    "# Local Minima\n",
    "#### Points where the cost function is lower than surrounding points but not the lowest overall. \n",
    "> #### Example: A valley within a mountain range that's not the deepest point.\n",
    "\n",
    "# Simultaneous Update in Gradient Descent\n",
    "#### Updating all parameters in the algorithm at once to ensure accurate direction towards the minimum. \n",
    "> #### Example: Adjusting both the slope and intercept of a line simultaneously to better fit data.\n",
    "\n",
    "# Assigning vs. Truth Assertion\n",
    "#### Assigning gives a variable a value, whereas truth assertion checks a condition's truth. \n",
    "> #### Example: Assigning speed = 100 vs. asserting speed == 100.\n",
    "\n",
    "# Gradient Descent Intuition\n",
    "#### Gradually minimizing error by adjusting parameters. \n",
    "> #### Example: Tweaking a recipe's ingredients slightly based on taste test feedback to improve the dish.\n",
    "\n",
    "# Running Gradient Descent\n",
    "#### Implementing the algorithm to adjust model parameters for minimal error. \n",
    "> #### Example: Iteratively modifying a car's speed and gear ratio for optimal fuel efficiency.\n",
    "\n",
    "# 'Batch' Gradient Descent\n",
    "#### Using the entire dataset to update parameters once per iteration. \n",
    "> #### Example: Calculating the average error from all house prices in a dataset before adjusting prediction factors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "#### Predicts outcomes based on multiple variables.\n",
    "> #### Example: Estimating a car's price from its age, mileage, and brand.\n",
    "\n",
    "# Vectorization with NumPy + `np.dot`\n",
    "#### Streamlines calculations over arrays.\n",
    "> #### Example: Calculating total sales by dot-multiplying quantities and prices arrays.\n",
    "\n",
    "# Gradient Descent for Multiple Linear Regression\n",
    "#### Adjusts multiple parameters to minimize prediction error.\n",
    "> #### Example: Optimizing both price and quality ratings in a product recommendation system.\n",
    "\n",
    "# Feature Scaling\n",
    "#### Normalizes feature ranges to enhance model training.\n",
    "> #### Example: Scaling income and age features before applying them in a loan eligibility prediction model.\n",
    "\n",
    "# Checking Gradient Descent for Convergence\n",
    "#### Monitoring error reduction to ensure optimal parameter values.\n",
    "> #### Example: Observing cost function decrease in iterations when training a model to predict electricity consumption based on temperature and humidity.\n",
    "\n",
    "# Choosing the Learning Rate\n",
    "#### Finding a balance to ensure efficient convergence without overshooting.\n",
    "> #### Example: Adjusting the learning rate to optimize the training speed of a stock price prediction model without causing volatility in predictions.\n",
    "\n",
    "# Feature Engineering\n",
    "#### Creating new, informative features from existing data.\n",
    "> #### Example: Deriving 'room area' from 'room length' and 'room width' to improve a housing price prediction model.\n",
    "\n",
    "# Polynomial Regression\n",
    "#### Modeling non-linear relationships by including polynomial terms.\n",
    "> #### Example: Predicting crop yields from rainfall, where yield increases to a point as rainfall increases, then decreases.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
