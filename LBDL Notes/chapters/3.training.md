# Training

Training is all about minimizing the loss and optimizing for the right set of weights for your function. The minimization of the loss is key here, involving mathematical and computational difficulties.

## Cross Entropy Loss
A very common loss function used in classification problems that measures the dissimilarity between the predicted probabilities of classes and the actual target labels.

## Soft Max
Used as activation functions in output layers of a neural network to convert raw scores (logits) into probability distributions.

## Metric Learning
A type of machine learning that focuses on learning distance metrics or similarity measures between data points.

## Contrastive Learning
Creates a pair of data points and then adjusts the embeddings so that the distance between similar pairs is minimized, and the distance between dissimilar pairs is maximized.

Usually, the loss minimized during training is not the actual quantity one wants to optimize but a proxy for which finding the model parameters is easier.

## Weight Decay
A technique in machine learning used to prevent overfitting models. It involves adding a penalty term to the loss function during training that encourages the model's weights to be smaller.
- This degrades performance on the training set but reduces the gap between the performance in training and that on new, unseen data.

## Autoregressive Model
A model that tries to predict what happens next based on what happened before.

## Discrete Sequence
A series of separate, distinct elements or values that are not continuous. Each element in the sequence is individual or specific.

## Causal Models
A way to understand and represent cause-and-effect relationships between variables. They help us make sense of how changes in one variable might influence other variables. They try to measure how things are connected.

## Tokenizer
The conversion to and from the token representation is done by this algorithm.
- **Byte Pair Encoding**: It's a game of combining letters to make new words. It's a way to compress representations of words into new symbols.

The tool of choice to minimize functions is called gradient descent.

## Gradient Descent
It's finding the best way downhill. What is going to get me where I want to go the fastest? It's used to optimize the parameters.
- It improves the estimate of parameters by iterating gradient steps, each consisting of computing the gradient of the loss with respect to the parameters.

## Learning Rate
A positive value that modulates how quickly the minimization is done and must be chosen carefully.

## Stochastic Gradient Descent
A faster version of gradient descent that uses random jumps to find the best path downhill.
- Imagine you're trying to go down a hill, but instead of taking one careful step at a time, you sometimes take a quick, random jump. These jumps may not be perfect, but they help you get down the hill faster.
- Start with a relatively large learning rate and make it smaller with each step. This is called a schedule.

## Adam Optimizer
An optimization algorithm used to update the parameters of the model during training. Start with bigger steps, and then go to smaller ones. Helps models converge to good solutions faster and with less manual tuning of parameters.

## Back Propagation (Back Prop)
Teach a model how to improve its predictions by fine-tuning its internal settings. Compare the model's predictions to the actual answers.

## Auto Grad
Autograd is an assistant that automatically calculates how changing one thing affects everything else in a complex formula.

Backward passes require twice the memory as a forward pass as you have to save the results of the forward pass to calculate the Jacobian.
- **Jacobian**: A collection of slopes that show how changing multiple things at once affects a bunch of other things.

The memory requirement during inference is roughly equal to that of the most demanding individual layer.

## Reversible Layers
Layers that can be put together and taken apart without losing any information. Used to retain all the original input information during forward and backward passes.

## Checkpointing
Saving your progress to continue from that point if you made a mistake.

## Vanishing Gradient
The gradient becomes smaller and smaller as more layers go through. This is a problem.

## Gradient Norm Clipping
A technique used during training to control the magnitude of gradients when they get too large.

Empirical evidence accumulated over 20 years demonstrates that state-of-the-art performance across application domains necessitates models with tens of layers.

## Residual Neural Networks
A type of deep neural network architecture designed to make training very deep networks more effective and efficient.

## Transformers
A type of architecture designed for processing sequences of data, particularly natural language.

Increasing the depth leads to a greater complexity of the resulting mapping.

Model size usually makes a better model. Kaplan scaling laws.