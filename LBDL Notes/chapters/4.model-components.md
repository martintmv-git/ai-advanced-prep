# Model Components

A deep model is nothing but a series of computations. Greater performance is achieved with deeper architectures and long compositions of mappings.

## Layers
Building blocks that process and transform data as it moves through a neural network. They represent different levels of abstraction and perform specific computations on the input data.

## Linear Layer
A layer that performs a linear transformation on the input data. Something like y = x.

## Affine
A combination of stretching, rotating, and moving things around on a coordinate plane.

## Fully Connected Layers
A type of layer where every neuron is connected to every neuron in the previous layer.

Gradient descent starts with all the parameters randomly initialized.

## Convolutional Layers
Specialized for processing grid-like data, such as images. Convolutional layers are designed to capture local patterns and relationships in the data.
- They can detect patterns in images.
- Filters are reusable patterns or templates that slide over an input image to highlight certain features. These are just a matrix, and you can determine the number of rows and columns.
- **Convolving**: Slide over each matrix in the image. You then calculate the dot product of the filter and the input matrix.
- Filters are commonly 3x3. The intensity of each filter is determined by backpropagation.

## Dot Product
A measure of how two things align in the same direction. It shows how similar two vectors are (the direction they point).
- a dot b is a1 * b1 + a2 * b2 + ... + an * bn

## Feature Map
A visualization of the responses of filters applied to an input image.

## Max Pool
Selects the maximum value in each region. It's a convolution but instead of calculating the dot product, you calculate the max() of each region.

## Average Pooling
Selects the average/mean value of a region.

## Padding
Adds another border around an image before applying filters to ensure that important information at the edges isn't lost. By default, there isn't padding.

Convolutions are used to recombine information, generally to reduce the spatial size of the representation, in exchange for a greater number of channels, which translates into a richer local representation.
- Map large-dimensional tensors to low-dimensional ones.

## Activation Function: ReLU
The most commonly used activation function is a ReLU function. It sets negative values to zero and keeps positive values unchanged.

Pooling basically summarizes the information.

## Dropout
Drops some neurons in a layer during each training iteration. This prevents overfitting by making the network less sensitive to the presence of any specific neuron. This is only during training, of course.

## Batch Normalization (Batch Norm)
Ensures the input to each layer of the network is centered and has a reasonable spread. This helps in faster training and improved convergence of the network.

## Skip Connections
Connections that allow information to bypass certain layers and be directly passed to subsequent layers.

## Attention Layers
Used to weigh different paths of input data differently, enabling the model to focus on relevant information.
- Address this problem by computing an attention score.

## Multi-head Attention
A technique used in attention mechanisms to enhance the model's ability to focus on various aspects of input data. It combines the attention of many heads.

## Self-attention
An operation that allows a model to weight different parts of input data based on their relationship within the same data.

## Cross-attention
This enables one part of the sequence to attend to another part.

## Embedding Layer
Converts categorical data into vectors that can be processed by the model.

## Positional Encoding
Provides information about the order of words in a sequence.