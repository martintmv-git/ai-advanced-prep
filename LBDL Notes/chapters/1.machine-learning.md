# Machine Learning

The simplest way to train a model is when you have inputs X that map to each output Y. This is known as **Supervised Learning**.
- By default, this is a challenging problem because devising an analytical recipe to map X in a high-dimensional space is difficult.

## Trainable Parameters
Coefficients or weights that a machine learning model adjusts during the training process to make predictions or classifications. These are represented as (w).
- The entire learning process in the model revolves around these parameters.

## Bias Term
The bias term adjusts for other factors that might affect the model.
- **Example**: Suppose you are trying to predict home prices based on size as an input. The price of a home at location X is higher than at location Y, even if they are the same size. The bias term (b) accounts for additional parameters like location, view, etc.
- It is updated during each run of backpropagation.

## Loss
The lower the loss, the better the function is at predicting the output. Minimizing the loss helps improve the model.

## Meta-Parameters
Models usually depend on meta-parameters.
- **Learning Rate**: The step size at which the function adjusts the weights and biases.
- **Batch Size**: Instead of updating the model's parameters after every single training example, which can be computationally expensive, training is done in smaller groups called batches. Larger batch sizes can lead to faster training but require more memory.
- **Hidden Layers**: Layers that sit between the input layer and the output layer. They are called hidden because they do not interact with the outside world.
- **Regularization Strength**: Used to avoid overfitting to complex data. It influences the optimization process.
- **Epoch**: Represents one complete pass through the entire training set during the training process. Too many epochs can lead to overfitting, while too few can result in underfitting.

## Mean Squared Error
Measures the average squared difference between the predicted values and the actual target values in the training data.

## Capacity of Model
The ability to represent and learn complex relationships within the data it's trained on. This indicates how diverse the model is. You want to avoid extremes, as they can lead to underfitting or overfitting.
- **Underfitting**: The model is too simple to understand the relationships, resulting in many errors.
- **Overfitting**: The model is too complex and not general enough for external data, only the provided dataset.

## Inductive Bias
Choosing the right structure of a deep learning model for the problem you are tackling.

## Categories of Models
- **Regression**: Best for predicting continuous numeric values. (Supervised)
- **Classification**: X maps to Y. (Supervised)
- **Density Modeling**: Estimates and models the probability distribution of a set of data points. Used to detect anomalies and different data points. (Unsupervised)

## Unsupervised Learning
The function learns without labeled data.
- **K-means Clustering**: Clusters data points into groups based on their features.

## Self-Supervised Learning
The model generates its own labels from the available data.