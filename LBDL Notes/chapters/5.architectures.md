# Architectures

## Multi-Layer Perceptron
Takes the form of a succession of fully connected layers separated by activation functions.

## Activation Functions
Without activation functions, the neural network would behave like a linear model and be limited in its ability to learn and represent intricate patterns in data. After each fully connected layer, you usually have an activation function like ReLU.

## Convolutional Networks
Used for processing images.

## Batch Normalization (Batch Norm)
Normalizes the output from the activation function so that all features can be on the same scale. It multiplies the normalized output by an arbitrary parameter g and adds an arbitrary parameter b to the resulting product.
- **Normalization**: Adjusting the values to a standard scale or range to make them more comparable and manageable.

## Transformer
A type of neural network architecture that introduced self-attention. Self-attention allows the model to weigh the importance of different words or tokens in a sequence when processing each token. This is excellent for handling long-range dependencies.
- **Encoder**: Processes the input sequence to get a refined representation.
- **Decoder**: An auto-regressor that generates each token of the result sequence, given the encoder's representation of the input sequence and the output tokens generated so far.

## Cross Attention
Used to condition a model with some more context information. Focuses on different parts of the source sequence while generating each element of the target sequence.

## GPT (Generative Pre-trained Transformer)
Scales really well with hundreds of billions of trainable parameters.

## Visual Transformer
A transformer that is designed specifically for processing and understanding visual data, such as images.