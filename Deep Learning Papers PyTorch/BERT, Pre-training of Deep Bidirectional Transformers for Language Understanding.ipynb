{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the \"[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\" paper.\n",
    "\n",
    "## Contents\n",
    "1. [Libraries](#libs)\n",
    "2. [Dataset](#dataset)\n",
    "\n",
    "    2.1. [Initializing The SpaCy English Tokenizer](#spacy)\n",
    "\n",
    "    2.2. [Creating a vocab](#vocab)\n",
    "\n",
    "    2.3. [Masked Language Modeling Dataset](#mlmdataset)\n",
    "\n",
    "3. [BERT](#bert)\n",
    "\n",
    "4. [Training session](#training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"libs\"></a>\n",
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Generator, Any\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from spacy.symbols import ORTH\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "\n",
    "# Controlling the randomness in PyTorch and NumPy.\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataset\"></a>\n",
    "## 2. The Dataset\n",
    "\n",
    "I am going to use the text from [Mary Shelley's Frankenstein](https://www.gutenberg.org/ebooks/84).\n",
    "Firstly, the whole text is saved into a string variable (not a scalable way of managing the data, but this corpus is really small). Then, I'll define a SpaCy Tokenizer. It will be used for the tokenization of the whole book. \n",
    "Afterwards, a Vocabulary is created based on the tokens.\n",
    "Finally, using the Tokenizer and the Vocabulary, a PyTorch Dataset is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join(\n",
    "    \"..\",\n",
    "    \"..\",\n",
    "    \"nlp\",\n",
    "    \"datasets\",\n",
    "    \"frankenstein\",\n",
    "    \"frankenstein.txt\"\n",
    ")\n",
    "book_text = open(DATASET_PATH, \"r\", encoding=\"utf8\").read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"spacy\"></a>\n",
    "### 2.1. Initializing The SpaCy English Tokenizer\n",
    "The SpaCy Tokenizer is updated with multiple special tokens:\n",
    "- `[CLS]` - classification token; used as a begining of sequence token\n",
    "- `[SEP]` - separation token; used for end of sequence and as a separation in NSP pre-training\n",
    "- `[MASK]` - mask token; used when the tokens are masked during MLM pre-training\n",
    "- `[PAD]` - padding token\n",
    "- `[UNK]` - unknown token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_TOKENIZER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "SPECIAL_TOKENS = [\n",
    "    \"[CLS]\",\n",
    "    \"[SEP]\",\n",
    "    \"[MASK]\",\n",
    "    \"[PAD]\",\n",
    "    \"[UNK]\"\n",
    "]\n",
    "# Adding a special rule for each special token.\n",
    "# If we don't do that, these tokens will be disregarded as such.\n",
    "for spec_token in SPECIAL_TOKENS:\n",
    "    rule = [{ORTH: spec_token}]\n",
    "    SPACY_TOKENIZER.tokenizer.add_special_case(spec_token, rule)\n",
    "\n",
    "def en_tokenizer(text):\n",
    "    return [token.orth_ for token in SPACY_TOKENIZER(text)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vocab\"></a>\n",
    "### 2.2. Creating a vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text: str) -> str:\n",
    "    \"\"\"Preparing the text for tokenization.\n",
    "    It includes:\n",
    "    1. Surrounding punctuation with whitespace\n",
    "    2. Converting multiple spaces into a single one\n",
    "\n",
    "    Args:\n",
    "        text (str): A text.\n",
    "\n",
    "    Returns:\n",
    "        str: The parsed text.\n",
    "    \"\"\"\n",
    "    pattern = r\"([.,!?:;]+)\"\n",
    "    text = re.sub(pattern, r\" \\1 \", text)\n",
    "\n",
    "    pattern = r\"\\s+\"\n",
    "    text = re.sub(pattern, \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_corpus(tokens: List[str], seq_size: int) -> Generator[str]:\n",
    "    \"\"\"Iterate through the corpus and yielding a batch of sequences.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): Word tokens.\n",
    "        seq_size (int): Length of a sequence batch.\n",
    "\n",
    "    Yields:\n",
    "        Generator[str]: Generated batch.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(tokens) - seq_size):\n",
    "        yield tokens[i:i + seq_size]\n",
    "\n",
    "\n",
    "book_text = prepare_text(book_text)\n",
    "tokens = en_tokenizer(book_text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    iterator=iterate_corpus(tokens, seq_size=100),\n",
    "    specials=SPECIAL_TOKENS\n",
    ")\n",
    "vocab.set_default_index(vocab[\"[UNK]\"])\n",
    "print(f\"Vocab size: {len(vocab)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mlmdataset\"></a>\n",
    "### 2.3. Masked Language Modeling Dataset\n",
    "\n",
    "Let's see what the authors of the paper said about MLM:\n",
    "> we simply mask some percentage of the input\n",
    "> tokens at random, and then predict those masked\n",
    "> tokens. We refer to this procedure as a “masked\n",
    "> LM” (MLM), although it is often referred to as a\n",
    "> Cloze task\n",
    "\n",
    "They then mask $15%$ of the tokens of each sequence:\n",
    "> In all of our experiments, we mask 15% of all WordPiece to-\n",
    "> kens in each sequence at random. \n",
    "\n",
    "While also having cases in which the `[MASK]` token is not used at all:\n",
    "> If the i-th token is chosen, we replace\n",
    "> the i-th token with (1) the [MASK] token 80% of\n",
    "> the time (2) a random token 10% of the time (3)\n",
    "> the unchanged i-th token 10% of the time.\n",
    "\n",
    "This $80$-$10$-$10$ configuration is added because that's pre-training. When the model\n",
    "is fine-tuned there may be many cases, in which we would not like to mask the input at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['[CLS]', 'Gutenberg', 'eBook', 'of', 'Frankenstein', ',', 'by', 'Mary', 'Wollstonecraft', 'Shelley', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', '[SEP]']\n",
      "Target: ['[CLS]', 'Gutenberg', 'eBook', 'of', 'Frankenstein', ',', 'by', 'Mary', 'Wollstonecraft', 'Shelley', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', '[SEP]']\n",
      "\n",
      "Input shape: torch.Size([3, 22])\n",
      "Target shape: torch.Size([3, 22])\n"
     ]
    }
   ],
   "source": [
    "class MlmDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        text: str, \n",
    "        tokenizer, \n",
    "        vocab, \n",
    "        seq_len: int, \n",
    "        mask_token=\"[MASK]\", masked_frac=0.15,\n",
    "        pad_token=\"[PAD]\"\n",
    "    ):\n",
    "        self._text = text\n",
    "        self._tokens = tokenizer(text)\n",
    "        self._vocab = vocab\n",
    "        self._itos = vocab.get_itos()\n",
    "        self._seq_len = seq_len\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._masked_frac = masked_frac\n",
    "        self._pad_token = pad_token\n",
    "\n",
    "        self.x, self.y = self._getinout()\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def _getinout(self) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"Get the input and output PyTorch tensors.\n",
    "        - 80% of the times replace 15% of the tokens with `self._mask_token`\n",
    "        - 10% of the times replace 15% of the tokens with random ones\n",
    "        - 10% of the times the input sequence is not changed\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.LongTensor, torch.LongTensor]: Input and output tensors.\n",
    "        \"\"\"\n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "        for i in range(0, len(self._tokens) - self._seq_len - 1):\n",
    "            sequence = self._tokens[i:i + self._seq_len]\n",
    "\n",
    "            # The random token indices that will be masked.\n",
    "            random_indices = random.sample(\n",
    "                range(0, len(sequence)),\n",
    "                int(len(sequence) * self._masked_frac)\n",
    "            )\n",
    "            random_indices.sort()\n",
    "\n",
    "            percentile = random.uniform(0, 1)\n",
    "\n",
    "            # 80% of the times - replace some of the tokens with [MASK].\n",
    "            if percentile <= 0.8:\n",
    "                replacement_tokens = [self._mask_token] * len(random_indices)\n",
    "\n",
    "            # 10% of the times - replace some of the tokens with random tokens.\n",
    "            elif percentile > 0.8 and percentile <= 0.9:\n",
    "                replacement_tokens = [\n",
    "                    self._itos[random.randint(0, len(self._vocab) - 1)] \n",
    "                    for _ in range(len(random_indices))\n",
    "                ]\n",
    "\n",
    "            # 10% of the times - don't change the sequence of tokens.\n",
    "            elif percentile > 0.9:\n",
    "                replacement_tokens = None\n",
    "\n",
    "            masked_tokens = [\"[CLS]\"] + sequence + [\"[SEP]\"]\n",
    "            sequence = [\"[CLS]\"] + self._replace_tokens(\n",
    "                sequence, random_indices, \n",
    "                replacement_tokens=replacement_tokens\n",
    "            ) + [\"[SEP]\"]\n",
    "\n",
    "            x.append([self._vocab[token] for token in sequence])\n",
    "            y.append([\n",
    "                self._vocab[token]\n",
    "                for token in masked_tokens\n",
    "            ])\n",
    "\n",
    "        return torch.LongTensor(x), torch.LongTensor(y)\n",
    "    \n",
    "    def _replace_tokens(self, sequence: List[str], indices: List[int], replacement_tokens: List[str]) -> List[str]:\n",
    "        \"\"\"Replace tokens in `sequence` with the `replacement_tokens`, based on `indices`.\n",
    "\n",
    "        Args:\n",
    "            sequence (List[str]): The input sequence.\n",
    "            indices (List[int]): The indices of the tokens that should be replaced.\n",
    "            replacement_tokens (List[str]): The tokens that are going to replace the original ones.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The transformed sequence.\n",
    "        \"\"\"\n",
    "        repl_token_i = 0\n",
    "\n",
    "        # If there are no replacement tokens, the sequence stays the same.\n",
    "        if replacement_tokens:\n",
    "            for i in range(len(sequence)):\n",
    "                if i in indices:\n",
    "                    sequence[i] = replacement_tokens[repl_token_i]\n",
    "                    repl_token_i += 1\n",
    "\n",
    "        return sequence\n",
    "    \n",
    "    def _pad_output(self, masked_sequence: List[str]) -> List[str]:\n",
    "        # Since this dataset is set up for MLM, there is no chance that the output\n",
    "        # sequence is longer than the input one (i.e. in the BERT paper, the MLM\n",
    "        # output sequence, without the padding, is 15% of the input length), we \n",
    "        # directly add padding.\n",
    "        padding_size = self._seq_len - len(masked_sequence)\n",
    "        return [\"[CLS]\"] + masked_sequence + [\"[SEP]\"] + [self._pad_token] * padding_size\n",
    "\n",
    "\n",
    "dataset = MlmDataset(\n",
    "    text=book_text, \n",
    "    tokenizer=en_tokenizer, \n",
    "    vocab=vocab,\n",
    "    seq_len=20\n",
    ")\n",
    "\n",
    "# Index to string\n",
    "ITOS = {vocab[token]: token for token in vocab.vocab.itos_}\n",
    "# String to index\n",
    "STOI = vocab\n",
    "\n",
    "batch = dataset[:10]\n",
    "\n",
    "x, y = batch[0][1].tolist(), batch[1][1].tolist()\n",
    "print(\"Input:\", [ITOS[el] for el in x])\n",
    "print(\"Target:\", [ITOS[el] for el in y])\n",
    "print()\n",
    "print(\"Input shape:\", dataset[:3][0].shape)\n",
    "print(\"Target shape:\", dataset[:3][1].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bert\"></a>\n",
    "## 3. BERT\n",
    "\n",
    "The BERT model uses only the Encoder of the classic Transformer architecture in the paper \"[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\" paper. Let's go through the architecture layer by layer.\n",
    "\n",
    "- *Embedding layers* - BERT uses three types of embedding layers. *Token Embedding* - used to embed the semantic meaning of the tokens, based on their usage; *Segment Embedding* - to denote when there are different segments in the input; *Position Embedding* - encodes the position of each token in the sequence\n",
    "\n",
    "- *Encoder layers* - Transformer Encoder layers. This is what the paper authors say about the configuration of these layers:\n",
    "    > We primarily report results on two model sizes:\n",
    "    > $BERT_{BASE}$ (L=12, H=768, A=12, Total Parameters=110M) and $BERT_{LARGE}$ (L=24, H=1024,\n",
    "    > A=16, Total Parameters=340M).\n",
    "\n",
    "- *Feed-forward layer* - Used to map the output Encoder Tensor to the expected output size (vocab size); for the NSP pre-training task, only the pooler output is used. Pooler output means that we only take the first element of the output vector, which corresponds to the `[CLS]` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5])\n",
      "Segmentations shape: torch.Size([2, 5])\n",
      "Output shape: torch.Size([2, 5, 100])\n"
     ]
    }
   ],
   "source": [
    "class Bert(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int, nheads: int, num_layers: int,\n",
    "        vocab_size: int,\n",
    "        max_seq_len=128,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % nheads == 0, \"'d_model' has to be divisible by 'nheads'.\"\n",
    "\n",
    "        # Token embedding\n",
    "        self.token_embed = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=d_model\n",
    "        )\n",
    "\n",
    "        # Segmentation embedding\n",
    "        # The difference here is that we are setting 'num_embeddings' to 3 because\n",
    "        # we will use 0 as padding index, 1 as sent1 index and 2 as sent2 index.\n",
    "        self.segm_embed = nn.Embedding(\n",
    "            num_embeddings=3, embedding_dim=d_model\n",
    "        )\n",
    "\n",
    "        self.pos_embed = nn.Embedding(\n",
    "            num_embeddings=max_seq_len, \n",
    "            embedding_dim=d_model\n",
    "        )\n",
    "\n",
    "        # Dropout after all embeddings\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nheads, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=enc_layer, num_layers=num_layers\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.project = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x: torch.LongTensor, segmentations: torch.LongTensor = None):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        seq_len = x.shape[1]\n",
    "        position_ids = torch.arange(seq_len, device=x.device).long()\n",
    "\n",
    "        # Input embedding\n",
    "        x = self.token_embed(x)\n",
    "        # Positional embeding\n",
    "        x += self.pos_embed(position_ids)\n",
    "        \n",
    "        # Optional segmentational encoding/embedding.\n",
    "        if segmentations is not None:\n",
    "            x += self.segm_embed(segmentations)\n",
    "\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        out = self.encoder(self.dropout1(x))\n",
    "        # out shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        out = self.project(out)\n",
    "        # out shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        (\n",
    "            torch.nn.init.uniform_(module, a=-initrange, b=initrange) \n",
    "            for module in self.encoder.modules()\n",
    "        )\n",
    "\n",
    "\n",
    "model = Bert(\n",
    "    d_model=64,\n",
    "    nheads=8,\n",
    "    num_layers=2,\n",
    "    vocab_size=100\n",
    ")\n",
    "\n",
    "x = torch.randint(low=0, high=99, size=(2, 5))\n",
    "segmentations = torch.randint(low=0, high=3, size=(2, 5))\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Segmentations shape:\", segmentations.shape)\n",
    "y_pred = model(x, segmentations=None)\n",
    "print(\"Output shape:\", y_pred.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"training\"></a>\n",
    "## 4. Training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert(\n",
      "  (token_embed): Embedding(7829, 768)\n",
      "  (segm_embed): Embedding(3, 768)\n",
      "  (pos_embed): Embedding(128, 768)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (6): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (7): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (8): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (9): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (10): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (11): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  (project): Linear(in_features=768, out_features=7829, bias=True)\n",
      ")\n",
      "Starting training session on device: cuda...\n",
      "Training session has finished!\n"
     ]
    }
   ],
   "source": [
    "class TrainingSession:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: nn.Module, \n",
    "        loss: nn.Module, \n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        itos: Dict[int, str],\n",
    "        device=\"cpu\"\n",
    "    ):\n",
    "        self._model = model.to(device)\n",
    "        self._loss_func = loss\n",
    "        self._opt = optimizer\n",
    "        self._itos = itos\n",
    "        self.device = device\n",
    "\n",
    "    def start(\n",
    "        self, \n",
    "        train_dataset: Dataset, valid_dataset: Dataset, \n",
    "        epochs: int, batch_size: int,\n",
    "        fixed_input: torch.LongTensor = None,\n",
    "        metrics: Dict[str, Any] = None,\n",
    "        save_model: bool = True,\n",
    "        model_path: str = \"./model.pt\"\n",
    "    ):\n",
    "        train_dl = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        valid_dl = DataLoader(\n",
    "            dataset=valid_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        self._metrics = metrics\n",
    "        self._metric_results = {}\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self._train_epoch(train_dl)\n",
    "            self._valid_epoch(valid_dl)\n",
    "\n",
    "            print(f\"Epoch: {epoch + 1}\")\n",
    "            print(self._metric_results)\n",
    "\n",
    "            if fixed_input is not None:\n",
    "                self._print_fixed_pred(fixed_input)\n",
    "\n",
    "            # Saving the model for the epoch.\n",
    "            if save_model:\n",
    "                print(f\"Saving model to '{model_path}'...\")\n",
    "                torch.save(self._model.state_dict(), model_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "    def _train_epoch(self, dataloader: DataLoader):\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "            y_pred = self._model(x)\n",
    "            self._opt.zero_grad()\n",
    "\n",
    "            y_pred = y_pred.reshape(-1, y_pred.shape[-1])\n",
    "            # y_pred shape: [batch_size * seq_len, vocab_size]\n",
    "            y = y.reshape(-1)\n",
    "            # y shape: [batch_size * seq_len]\n",
    "\n",
    "            loss = self._loss_func(y_pred, y)\n",
    "            loss.backward()\n",
    "\n",
    "            self._opt.step()\n",
    "\n",
    "        # Adding the loss to the metrics.\n",
    "        self._metric_results[\"Training Loss\"] = loss.item()\n",
    "        self._metric_results = self._calc_metrics(y_pred, y, type_=\"Training\")\n",
    "\n",
    "        return self._metric_results\n",
    "\n",
    "    def _valid_epoch(self, dataloader: DataLoader):\n",
    "        self._model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in dataloader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                y_pred = self._model(x)\n",
    "\n",
    "                y_pred = y_pred.reshape(-1, y_pred.shape[-1])\n",
    "                y = y.reshape(-1)\n",
    "\n",
    "                loss = self._loss_func(y_pred, y)\n",
    "\n",
    "        self._model.train()\n",
    "\n",
    "        self._metric_results[\"Validation Loss\"] = loss.item()\n",
    "        self._metric_results = self._calc_metrics(y_pred, y, type_=\"Validation\")\n",
    "\n",
    "        return self._metric_results\n",
    "\n",
    "    def _print_fixed_pred(self, fixed_input: torch.LongTensor):\n",
    "        self._model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = self._model(fixed_input.to(self.device))\n",
    "            pred = pred.argmax(-1)\n",
    "\n",
    "        print(f\"Input: {[self._itos[int(idx)] for idx in fixed_input[0]]}\")\n",
    "        print(f\"Prediction {[self._itos[int(idx)] for idx in pred[0]]}\")\n",
    "\n",
    "        self._model.train()\n",
    "\n",
    "    def _calc_metrics(self, y_pred: torch.Tensor, y: torch.LongTensor, type_: str):\n",
    "        for name, metric in self._metrics.items():\n",
    "            self._metric_results[f\"{type_} {name}\"] = float(metric(y_pred, y))\n",
    "\n",
    "        return self._metric_results\n",
    "\n",
    "\n",
    "# Dataset params:\n",
    "TRAIN_FRAC = 0.8\n",
    "SEQ_LEN = 20\n",
    "FIXED_INPUT = en_tokenizer(\"[CLS] Time is [MASK] [SEP]\")\n",
    "FIXED_INPUT = [\n",
    "    STOI[token]\n",
    "    for token in FIXED_INPUT\n",
    "]\n",
    "FIXED_INPUT = torch.LongTensor(FIXED_INPUT).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Model hyperparams:\n",
    "# One of the learning rates recommended in the paper.\n",
    "L_RATE = 3e-5\n",
    "EPS = 1e-8\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAVE_MODEL = True\n",
    "MODEL_FILENAME = \"2-bert.pt\"\n",
    "MODEL_PATH = os.path.join(\"..\", \"models\", MODEL_FILENAME)\n",
    "\n",
    "HIDDEN_SIZE = 768\n",
    "NHEADS = 12\n",
    "LAYERS = 12\n",
    "\n",
    "dataset = MlmDataset(\n",
    "    text=book_text, \n",
    "    tokenizer=en_tokenizer, \n",
    "    vocab=vocab,\n",
    "    seq_len=SEQ_LEN\n",
    ")\n",
    "train_size = int(TRAIN_FRAC * len(dataset))\n",
    "\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    dataset=dataset,\n",
    "    lengths=[train_size, len(dataset) - train_size]\n",
    ")\n",
    "\n",
    "\n",
    "bert = Bert(\n",
    "    d_model=HIDDEN_SIZE,\n",
    "    nheads=NHEADS,\n",
    "    num_layers=LAYERS,\n",
    "    vocab_size=len(vocab)\n",
    ")\n",
    "print(bert)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=L_RATE, eps=EPS)\n",
    "loss = nn.CrossEntropyLoss(ignore_index=vocab[\"[PAD]\"])\n",
    "\n",
    "\n",
    "session = TrainingSession(\n",
    "    model=bert,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    itos=ITOS,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(f\"Starting training session on device: {DEVICE}...\")\n",
    "# TODO: Use a larger dataset for the pretraining process.\n",
    "# Currently I use a small dataset\n",
    "# session.start(\n",
    "#     train_dataset=train_dataset,\n",
    "#     valid_dataset=valid_dataset,\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     fixed_input=FIXED_INPUT,\n",
    "#     metrics={\n",
    "#         \"Accuracy\": Accuracy(\n",
    "#             task=\"multiclass\",\n",
    "#             num_classes=len(vocab)\n",
    "#         ).to(DEVICE)\n",
    "#     },\n",
    "#     save_model=SAVE_MODEL,\n",
    "#     model_path=MODEL_PATH\n",
    "# )\n",
    "print(\"Training session has finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8100bb27ef6f27bb6b63ba202e13f32f0dffed430e6a4d162d3986e448f218b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
